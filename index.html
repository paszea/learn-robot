<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Software Engineer's Adventure in Robot Land</title>
    <style>
        /* Styles for the body to remove default margins */
        body {
            margin: 0;
            font-family: sans-serif;
            background-color: #f4f4f4; /* Light background for the full page */
        }

        /* Styles for the main content container */
        .container {
            max-width: 960px; /* Your desired maximum width */
            margin: 0 auto;   /* Center the container horizontally */
            padding: 20px;    /* Add some space around the content */
            background-color: #ffffff; /* White background for the content area */
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1); /* Optional: subtle shadow */
            box-sizing: border-box; /* Include padding in the element's total width */
        }

        /* Basic styling for demonstration content */
        h1 {
            color: #0b5394;
        }

        p {
            line-height: 1.6;
            color: #555;
        }
    </style>
</head>
<body>
<div class="container">

<h1>A Software Engineer's Adventure in Robot Land</h1>

<p style="color: #0b5394;"><i>06/17/2025</i></p>
<p>I know coding. But robots are always a mystery to me. That started to change in early April 2025 when I built my first robot arm (so100) two weeks after I learned about <a href="https://github.com/huggingface/lerobot">LeRobot</a> open source project. In the two and half months that followed, I felt like a child in a candy shop. I tinkered and experimented. I learned robotics and built demos and projects that I could only dream of at the start of April 2025. Below is a brief account of this journey.
</p>

<p>I trained my first VLA model, ACT, soon after the arm was built. What a magic moment it was to watch a lifeless arm sprang into life seeking and grasping!
</p>
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/zxb1YeRxSYo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

<p>
Eager for more, I immediately moved to a harder challenge: multi-lego pick-n-place. ACT couldn't cut it. But two other powerful models did: first Groot N1.
</p>
<div class="video-container">
    <iframe width="560" height="325" src="https://www.youtube.com/embed/dLe7exy6cZU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<p>
Then Pi0. Pi0 was harder to train than N1. But it performed better than N1. The arm was less shaky and more precise under Pi0.
</p>
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/JrbZqQxib5k" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<p>
Having had a taste of success, I ordered more parts and upgraded so100 to a mobile LeKiwi. By Easter, I trained the LeKiwi to hunt for eggs using Pi0.
</p>
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/4a2rNi7PvBs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<p>
Challenges now escalated to games! And it gave me the first opportunity to use LLM in a robot project. I called Gemini to help the robot come up with moves to win a game.
</p>
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/MpV-O8Q5-x4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<p>
Yet at this point I didn't really know how to effectively train a VLA to control the robot to pick or place at a specific location especially when there are a large number of combinations. The usual language conditioning of ACT, Pi0 and N1, in my experiments, was not quite up to the task, at least not without a huge amount of data. As a result I resorted to manual prescription of a set of primitives, e.g. pick the first lego, put to top-right, etc. How to automate them using a model remained a struggle for me for a while.
</p>
<p>
The struggle led me to conduct a series of experiments which led to an epiphany discovery of using bounding boxes (bbox) to condition ACT for precise control and accuracy (<a href="https://discord.com/channels/1216765309076115607/1237738269500899400/1367605706496282725">Details</a>):
</p>
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/dMTH0zFvFww" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<p>
I was quite excited by the discovery. It enabled me to easily build an agentic robot demo like the following. The robot chatted and fetched what's asked of it, and could recover from its failures.
</p>
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/eOH3eSNxkfg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

<p>
Agentic and autonomous robots are always facinating especially if you combine locomotion and manipulation. I developed another demo that had LeKiwi to search the floor for legos and pick them up. 
</p>
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/6FK6Pl8VYmI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

<p>
Seeing the success of bbox, I started to wonder whether images, which are the standard input to VLA, are really necessary in the presence of bbox? Could models do away with them? That led to the development of "Blind ACT".
</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Blind ACT - a poor man&#39;s IK<br><br>How do you like an ACT model that grasps like a pro, but is 4x as fast and invariant to scene changes?<br><br>The so100 in the demo is controlled by a policy that sees nothing. The image on the laptop is for humans to visualize. The robot acts blind. 1/n <a href="https://t.co/S8WgyAARyO">pic.twitter.com/S8WgyAARyO</a></p>&mdash; xun (@paszea) <a href="https://twitter.com/paszea/status/1924872831378358272?ref_src=twsrc%5Etfw">May 20, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>
ACT without images trains 16x faster! It made a big difference in my experiment turnover while providing pricise control of the robot beheaviors. I further introduced 3D signals (bboxes from two images) in a model, and trained my robot arm to become an expert at stacking blocks.
</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Get multiview correspondence. Ditch 3d coordinates.<br><br>Using a VLM like gemini, you can perform multiview correspondence to extract bbox&#39;es from two images for an object.<br><br>Now throw away the raw images and feed the bbox&#39;es to a blind ACT model.<br><br>You have a stack master. 1/ <a href="https://t.co/HSHqxpVzVZ">pic.twitter.com/HSHqxpVzVZ</a></p>&mdash; xun (@paszea) <a href="https://twitter.com/paszea/status/1927769275080524031?ref_src=twsrc%5Etfw">May 28, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>
I've been reading robotics papers along the way. At this point, I came across a <a href="http://arxiv.org/abs/2403.19578">paper</a> and realized the bbox is one type of a broader concept called keypoints. Great news! It meant I could try LLM in-context learning as discussed in the paper, which led to this experiment. 
</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">In-context learning to control so100<br><br>Give gemini-2.5-flash a few examples. It learned to control my so100: <a href="https://t.co/c03bCbUgPo">pic.twitter.com/c03bCbUgPo</a></p>&mdash; xun (@paszea) <a href="https://twitter.com/paszea/status/1929659534517354870?ref_src=twsrc%5Etfw">June 2, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>
In-context learning still requires a few demonstrations. Could we do better without any demonstrations, as some Gemini Robotics demos did, zero-shot? Zero-shot sounded nice!
</p>
<p>
To get zero-shot, I first had an idea of developing a simplified IK model to move the robot's gripper to a given point in images. This simplified IK model was trained using data, not hand-crafted, and required no coded math calculation or measurement. This X post explains how it's done.
</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">So100 costs $110. I has no fancy features. You control it by setting its 6-joint positions.<br><br>But you can develop a fancy feature, like a simplified IK to move the arm to an arbitrary position in images. See <a href="https://t.co/1ZzuOJEs4M">pic.twitter.com/1ZzuOJEs4M</a></p>&mdash; xun (@paszea) <a href="https://twitter.com/paszea/status/1932559764241994197?ref_src=twsrc%5Etfw">June 10, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>
With the help of the IK model, as well as Gemini Live, I could finally have a "Hi Robot" moment for my so100 without any demonstration or training.
</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/zeroshot?src=hash&amp;ref_src=twsrc%5Etfw">#zeroshot</a> <a href="https://twitter.com/hashtag/so100?src=hash&amp;ref_src=twsrc%5Etfw">#so100</a><br>A robot that talks too much and pauses too often, yet performs tasks Zero Shot. No demonstration needed.<br><br>1/ Put things in a plate <a href="https://t.co/2Z26c4XUI9">pic.twitter.com/2Z26c4XUI9</a></p>&mdash; xun (@paszea) <a href="https://twitter.com/paszea/status/1933280906137240010?ref_src=twsrc%5Etfw">June 12, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>
Below is another zero-shot demo where Gemini figured out "a point in front of Kerby" to put the candy. By harvesting the power of a "System 2", it saved much of manual chores (teleoperation) for me.
</p>
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/vGwoSGFgL0Y" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

<p>
As a software engineer generalist, I found it very amusing to watch pre-AI era robots playing soccer so awkwardly. Robotics was hard! But things are changing fast. Impressive robot demos now pop up every day on Internet. Even my dreaming cloth folding robot has become a reality as demoed by Physical Intelligence, the company behind Pi0.
</p>
<p>You could almost smell in the air that a robotic future is coming at us fast:  robots, all shapes and sizes, living with us in our homes, shops and offices, helping perform all kinds of useful tasks. As I worked on these projects during the two-and-half-month journey, I couldn't help but feel we are at the cusp of this vision becoming reality.
</p>

<h3>Epilogue</h3>
<p>
I'd like to give a big shout out to LeRobot for making robotics easily accessible. It's a game changer. Anyone with coding skills can now experience and learn the latest advancements in robotics.  
</p>
<p>
And the end of this blog is not the end of my robotics journey. The journey will certainly continue. Innovative ideas and discoveries pop up every day. There are so many things to explore. For example, reinforcement learning, especially HIL-SERL showed great promise in making the model robust and precise. It's now supported by LeRobot. I'm eager to see how far it can take so100 along this route.
</p>

</div>
</body>
</html>
