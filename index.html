<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Software Engineer's Journey into Robotics</title>
    <style>
        /* Styles for the body to remove default margins */
        body {
            margin: 0;
            font-family: sans-serif;
            background-color: #f4f4f4; /* Light background for the full page */
        }

        /* Styles for the main content container */
        .container {
            max-width: 960px; /* Your desired maximum width */
            margin: 0 auto;   /* Center the container horizontally */
            padding: 20px;    /* Add some space around the content */
            background-color: #ffffff; /* White background for the content area */
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1); /* Optional: subtle shadow */
            box-sizing: border-box; /* Include padding in the element's total width */
        }

        /* Basic styling for demonstration content */
        h1 {
            color: #0b5394;
        }

        p {
            line-height: 1.6;
            color: #555;
        }

        i {
            color: #555;
        }
    </style>
</head>
<body>
<div class="container">

<h1>A Software Engineer's Journey into Robotics</h1>

<i><p style="color: #0b5394;">06/18/2025</p></i>
<p>I know coding. But robots have always been a mystery to me. That started to change in April after I built the first robot arm that I have ever touched. In the months that followed, I tinkered with it one project after another. And before long I found myself building demos I could have only dreamed of before April. 
</p>
<p>How is this possible? Robotics is going through a ground shift at the moment. Things that were impossible a few years ago become easy. And LeRobot, through its low cost open source project, made it possible for anyone with coding skills to experience this great shift themselves. I am grateful for having embarked on the journey. 
It's one of the greatest learning experiences for me. And I want to share it with you here 
</p>
    
<h3>The journey</h3>
<p>I trained my first VLA model, ACT, soon after the arm was built. What a magic moment it was to watch a lifeless arm sprang into life seeking and grasping!
</p>
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/zxb1YeRxSYo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<i>ACT is small, fast and effective. Unfortunately it needs cuda for training (on mac mps the loss blows up to inf/nan.) Google Colab is a cheap and convenient place to get cuda. I trained all my models there: ACT on L4, Pi0/Groot N1 on A100(40G).
<a href="https://huggingface.co/spaces/lerobot/visualize_dataset?path=%2Fpaszea%2Fso100_lego">Dataset</a></i>

<p>
Eager for more, I quickly moved to a harder challenge: multi-lego pick-n-place. ACT couldn't cut it. But two other powerful models did. First Groot N1.
</p>
<div class="video-container">
    <iframe width="560" height="325" src="https://www.youtube.com/embed/dLe7exy6cZU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<i>One overhead camera. Finetuned for 20k steps (2 hours.) Pretty impressive it worked so well. I also finetuned Pi0 for 20k steps using the same data but couldn't get it to work nearly as well. Maybe I didn't train it long enough.
<a href="https://huggingface.co/spaces/lerobot/visualize_dataset?path=%2Fpaszea%2Fso100_lego_mix">Dataset</a></i>
<p>
Then Pi0. Pi0 was harder to train than N1. But it performed better than N1 if you could get it to work. The arm was less shaky and more precise under Pi0.
</p>
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/JrbZqQxib5k" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<i>This time with two cameras and trained for long (40k+ steps), Pi0 worked really well.
Surprisingly N1 didn't do well in this 2 camera setting.
<a href="https://huggingface.co/spaces/lerobot/visualize_dataset?path=%2Fpaszea%2Fso100_lego_2cam">Dataset</a></i>
<p>
Having had a taste of success, I ordered more parts and upgraded so100 to a mobile LeKiwi. By Easter, I trained the LeKiwi to hunt for eggs using Pi0.
</p>
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/4a2rNi7PvBs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<i>The model combined locomotion and manipulation. It's much harder to train. Pi0 had a low success rate. And it didnt always follow instructions to go for the correct colored egg despite it's task-conditioned in training.
 Another interesting thing was that the model could not sit still. It consistently made small movements to the wheels that made it hard for the arm to manipulate.
And N1 completely failed on this task.
<a href="https://huggingface.co/spaces/lerobot/visualize_dataset?path=%2Fpaszea%2Feaster_2">Dataset</a></i>
<p>
Challenges now escalated to games! And it gave me the first opportunity to use LLM in a robot project. I called Gemini to help robot come up with moves by giving it snapshots of the game board.
</p>
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/MpV-O8Q5-x4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<p>
Yet at this point I didn't really know how to effectively train a VLA to control the robot to pick or place at a specific location especially when there are many combinations. The usual language conditioning of ACT, Pi0 and N1, in my experiments, was not quite up to the task, at least not without a large amount of data. As a result I resorted to manual prescription of a set of primitives, e.g. pick the first lego, put to top-right, etc. How to automate them using a model remained a struggle for me for a while.
</p>
<p>
The struggle led me to conduct a series of experiments which led to an epiphany discovery of using bounding boxes (bbox) to condition ACT for precise control and accuracy <i>(<a href="https://discord.com/channels/1216765309076115607/1237738269500899400/1367605706496282725">Details</a>)</i>:
</p>
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/dMTH0zFvFww" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<i>I started with 20 episodes as poc and increased the size to 60 episodes which made the model reallly good. Then the moving jaw of so100 broke down and was replaced. I had to collect another 10 episodes to finetune the model to account for the differences caused by the new jaw. 
    <a href="https://huggingface.co/spaces/lerobot/visualize_dataset?path=%2Fpaszea%2Flekiwi_aug_grab">Dataset</a></i>
<p>
I was quite excited by the discovery for the model also generalized. It enabled me to easily build an agentic robot demo like the following. The robot chatted and fetched what's asked of it, and could recover from its failures.

<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/eOH3eSNxkfg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<i>Underneath was an agentic flow powered by gemini live and two simple apis, detect_objects and pick_object, for bbox detection and model inference respectively.</i>
</p>

<p>
Agentic and autonomous robots are always facinating especially if you combine locomotion and manipulation. I developed another demo that had LeKiwi search the floor for legos to pick up. 
</p>
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/6FK6Pl8VYmI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<i>I didnt bother to train a locomotion model. I simply asked gemini to go around in a specific order to find legos.</i>
<p>
Seeing the success of bbox, I started to wonder whether images, which are the standard input to VLA, are really necessary in the presence of bbox? Could models do away with them? That led to the development of "Blind ACT".
</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Blind ACT - a poor man&#39;s IK<br><br>How do you like an ACT model that grasps like a pro, but is 4x as fast and invariant to scene changes?<br><br>The so100 in the demo is controlled by a policy that sees nothing. The image on the laptop is for humans to visualize. The robot acts blind. 1/n <a href="https://t.co/S8WgyAARyO">pic.twitter.com/S8WgyAARyO</a></p>&mdash; xun (@paszea) <a href="https://twitter.com/paszea/status/1924872831378358272?ref_src=twsrc%5Etfw">May 20, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<i>See X threads for details and datasets</i>
<p>
ACT without images trains 16x faster! It made a big difference in my experiment turnover while still providing precise control of the robot beheaviors. I further introduced 3D signals (bboxes from two images) in a model, and trained my robot arm to become an expert at stacking blocks.
</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Get multiview correspondence. Ditch 3d coordinates.<br><br>Using a VLM like gemini, you can perform multiview correspondence to extract bbox&#39;es from two images for an object.<br><br>Now throw away the raw images and feed the bbox&#39;es to a blind ACT model.<br><br>You have a stack master. 1/ <a href="https://t.co/HSHqxpVzVZ">pic.twitter.com/HSHqxpVzVZ</a></p>&mdash; xun (@paszea) <a href="https://twitter.com/paszea/status/1927769275080524031?ref_src=twsrc%5Etfw">May 28, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>
I've been reading robotics papers along the way. At this point, I came across a <a href="http://arxiv.org/abs/2403.19578">paper</a> and realized the bbox is one type of a broader concept called keypoints. Great news! It meant I could try LLM in-context learning idea as discussed in the paper, which led to this experiment. 
</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">In-context learning to control so100<br><br>Give gemini-2.5-flash a few examples. It learned to control my so100: <a href="https://t.co/c03bCbUgPo">pic.twitter.com/c03bCbUgPo</a></p>&mdash; xun (@paszea) <a href="https://twitter.com/paszea/status/1929659534517354870?ref_src=twsrc%5Etfw">June 2, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>
In-context learning still requires a few demonstrations. Could we do better without any demonstrations, as some Gemini Robotics demos did, zero-shot? Zero-shot sounded nice!
</p>
<p>
To get zero-shot, I first had the idea of developing a simplified IK model to move the robot's gripper to any given point in images. This simplified IK model was trained using data, not hand-crafted, and required no coded math calculation or measurement. This X post explains how it's done.
</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">So100 costs $110. I has no fancy features. You control it by setting its 6-joint positions.<br><br>But you can develop a fancy feature, like a simplified IK to move the arm to an arbitrary position in images. See <a href="https://t.co/1ZzuOJEs4M">pic.twitter.com/1ZzuOJEs4M</a></p>&mdash; xun (@paszea) <a href="https://twitter.com/paszea/status/1932559764241994197?ref_src=twsrc%5Etfw">June 10, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>
With the help of the IK model and Gemini Live, I could finally have a "Hi Robot" moment for my so100 without any demonstration or training.
</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/zeroshot?src=hash&amp;ref_src=twsrc%5Etfw">#zeroshot</a> <a href="https://twitter.com/hashtag/so100?src=hash&amp;ref_src=twsrc%5Etfw">#so100</a><br>A robot that talks too much and pauses too often, yet performs tasks Zero Shot. No demonstration needed.<br><br>1/ Put things in a plate <a href="https://t.co/2Z26c4XUI9">pic.twitter.com/2Z26c4XUI9</a></p>&mdash; xun (@paszea) <a href="https://twitter.com/paszea/status/1933280906137240010?ref_src=twsrc%5Etfw">June 12, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>
Below is another zero-shot demo where Gemini figured out "a point in front of Kerby" to put the candy. By harvesting the power of a "System 2", it saved much of manual chores (teleoperation) for me.
</p>
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/vGwoSGFgL0Y" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

<p>
Robotics was hard. I used to find it quite amusing to watch pre-AI era robots playing soccer. They were spectacularly clumsy. But things are changing fast. Impressive robot demos now pop up every day on Internet. 
    
All terain robot dogs, dancig humanoids, cloth folding, picnic packing, kitchen cleaning, ..., you name it.
</p>
<p>You could almost smell in the air that a robotic future is coming at us fast:  robots, all shapes and sizes, living with us in our homes, shops and offices, helping perform all kinds of useful tasks. As I worked on these projects during the two-and-half-month journey, I couldn't help but feel we are at the cusp of this vision becoming reality.
</p>

<h3>A final note</h3>
<p>
I'd really like to give a big shout out to <a href="https://github.com/huggingface/lerobot">LeRobot</a> team for making robotics easily accessible. It's truely a game changer. Anyone with coding skills can now experience and learn the latest advancements in robotics.  
</p>
<p>
And the end of this blog is certainly not the end of my robotics journey. It rather marks the start of the journey - there are so many more great ideas waiting to be explored.

</div>
</body>
</html>
