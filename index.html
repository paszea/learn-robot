---
layout: post
title: "A Software Engineer's Robotics Journey"
date: 2025-06-16 10:00:00 -0700
categories: [blogging, github]
---

<h2>A Software Engineer's Journey to Robotics</h2>

I know coding. But robots are always a mystery to me until early April 2025, two weeks after I learned about <a href="https://github.com/huggingface/lerobot">LeRobot</a> open source project, I built a so100 arm. In the two and half months that followed, I felt like a child in a candy shop. I tinkered and experimented. I learned robotics and built demos and projects that I could only dreamed of at the start of April 2025. Below is a brief account of this journey.
<p>
I trained my first VLA model, ACT, soon after the arm was built. What a magic moment is it to watch a lifeless arm sprang into life seeking and grasping!
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/zxb1YeRxSYo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

<br>
Eager for more, I immediately moved to a harder challenge: multi-lego pick-n-place. ACT couldn't cut it. But two other powerful models could: first Groot N1.
<div class="video-container">
    <iframe width="560" height="325" src="https://www.youtube.com/embed/dLe7exy6cZU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<br>
Then Pi0. Pi0 was harder to train than N1. But it performed better than N1. The arm was less shaky and more precise under Pi0.
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/JrbZqQxib5k" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<p>
Having had a taste of success, I ordered more parts and upgraded so100 to a mobile LeKiwi. By Easter, I trained the LeKiwi to hunt for eggs using Pi0.
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/4a2rNi7PvBs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<br>
Challenges now escalated to games! And it gave me the first opportunity to use LLM in a robot project. I called Gemini to help the robot come up with moves to win a game.
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/MpV-O8Q5-x4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<p>
Yet at this point I didn't really know how to effectively train a VLA to control the robot to pick or place at a specific location especially when there are a large number of combinations. The usual language conditioning of ACT, Pi0 and N1, in my experiments, was not quite up to the task, at least not without a huge amount of data. As a result I resorted to manual prescription of a set of primitives, e.g. pick the first lego, put to top-right, etc. How to automate them using a model remained a struggle for me for a while.
<p>
The struggle led me to conduct a series of experiments which led to an epiphany discovery of using bounding boxes (bbox) to condition ACT for precise control and accuracy:
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/dMTH0zFvFww" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<br>
I was quite excitied about the discovery. It enabled me to easily build an agentic robot demo like the following. The robot chatted and fetched what's asked of it, and could recover from its failures.
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/eOH3eSNxkfg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

<br>
Agentic and autonomous robots are always facinating especially if you combine locomotion and manipulation. I developed another demo that had LeKiwi to search the floor for legos and pick them up. 
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/6FK6Pl8VYmI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

<p>
Seeing the success of bbox, I started to wonder whether images, which are the standard input to VLA, are really necessary in the presence of bbox? Could models do away with them? That led to the development of "Blind ACT".
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Blind ACT - a poor man&#39;s IK<br><br>How do you like an ACT model that grasps like a pro, but is 4x as fast and invariant to scene changes?<br><br>The so100 in the demo is controlled by a policy that sees nothing. The image on the laptop is for humans to visualize. The robot acts blind. 1/n <a href="https://t.co/S8WgyAARyO">pic.twitter.com/S8WgyAARyO</a></p>&mdash; xun (@paszea) <a href="https://twitter.com/paszea/status/1924872831378358272?ref_src=twsrc%5Etfw">May 20, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>
ACT without images trains 16x faster! It made a big difference in my experiment turnover. I further introduced 3D signals (bboxes from two images) in a model, and trained my robot arm to become an expert at stacking blocks.
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Get multiview correspondence. Ditch 3d coordinates.<br><br>Using a VLM like gemini, you can perform multiview correspondence to extract bbox&#39;es from two images for an object.<br><br>Now throw away the raw images and feed the bbox&#39;es to a blind ACT model.<br><br>You have a stack master. 1/ <a href="https://t.co/HSHqxpVzVZ">pic.twitter.com/HSHqxpVzVZ</a></p>&mdash; xun (@paszea) <a href="https://twitter.com/paszea/status/1927769275080524031?ref_src=twsrc%5Etfw">May 28, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>I've been reading robotics papers along the way. At this point, I came across a <a href="http://arxiv.org/abs/2403.19578">paper</a> and realized the bbox is one type of a broader concept called keypoints. Great news! It meant I could try LLM in-context learning as discussed in the paper, which led to this experiment. 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">In-context learning to control so100<br><br>Give gemini-2.5-flash a few examples. It learned to control my so100: <a href="https://t.co/c03bCbUgPo">pic.twitter.com/c03bCbUgPo</a></p>&mdash; xun (@paszea) <a href="https://twitter.com/paszea/status/1929659534517354870?ref_src=twsrc%5Etfw">June 2, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>In-context learning still requires a few demonstrations. Could we do better without any demonstrations, as some Gemini Robotics demos did, zero-shot? Zero-shot sounded nice!

<p>
To get zero-shot, I first had an idea of developing a simplified IK model to move the robot's gripper to a given point in images. This simplified IK model was trained using data, not hand-crafted, and required no explicit math calculation or measurement. This X post explains how it's done.
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">So100 costs $110. I has no fancy features. You control it by setting its 6-joint positions.<br><br>But you can develop a fancy feature, like a simplified IK to move the arm to an arbitrary position in images. See <a href="https://t.co/1ZzuOJEs4M">pic.twitter.com/1ZzuOJEs4M</a></p>&mdash; xun (@paszea) <a href="https://twitter.com/paszea/status/1932559764241994197?ref_src=twsrc%5Etfw">June 10, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<br>
With the help of the IK model, as well as Gemini Live, I could finally have a "Hi Robot" moment for my so100 without any demonstration or training.
<blockquote class="twitter-tweet"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/zeroshot?src=hash&amp;ref_src=twsrc%5Etfw">#zeroshot</a> <a href="https://twitter.com/hashtag/so100?src=hash&amp;ref_src=twsrc%5Etfw">#so100</a><br>A robot that talks too much and pauses too often, yet performs tasks Zero Shot. No demonstration needed.<br><br>1/ Put things in a plate <a href="https://t.co/2Z26c4XUI9">pic.twitter.com/2Z26c4XUI9</a></p>&mdash; xun (@paszea) <a href="https://twitter.com/paszea/status/1933280906137240010?ref_src=twsrc%5Etfw">June 12, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<br>
Below is another zero-shot demo where Gemini figured out "a point in front of Kerby." By harvesting the power of a "System 2", it saved much manual chores (teleoperation) for me, and will undoubtedly do in future for all of us.
<div class="video-container">
    <iframe width="325" height="560" src="https://www.youtube.com/embed/vGwoSGFgL0Y" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

<br><p>
As a software engineer generalist, I found it very amusing to watch pre-AI era robots playing soccer so awkwardly. Robotics was hard! But things have changed. Impressive robot demos pop up every day on Internet. Even my dreaming cloth folding robot is now a reality as demoed by Physical Intelligence, the company behind Pi0. You could almost smell in the air that a robotic future is coming at us fast:  robots, all shapes and sizes, living with us everywhere, in our homes, in our factories, helping us perform all kinds of useful work. As I worked on these projects and demos during the two-and-half-month journey,  I can't help but feel we are getting closer to this future each and every day.
<p>

<h3>Epilogue</h3>
A big shout out to LeRobot for making robotics easily accessible to everyone. It's a game changer. Everyone with some coding skills can experience and learn the latest advancements in robotics.  
<p>
And the end of this blog is not the end of my robotics journey. The journey will certainly continue. Innovative ideas and discoveries pop up every day. There are so many things to explore. For example, reinforcement learning, especially HIL-SERL showed great promise in making the model robust and precise. I'm eager to see how far it can take so100 along this route.
